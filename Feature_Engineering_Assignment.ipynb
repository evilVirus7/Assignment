{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. What is a parameter? ###\n",
        "\n",
        "Ans - A parameter in machine learning is a variable that a model learns from data and uses to make predictions. Parameters are internal to the model and are adjusted during training to improve the model's accuracy."
      ],
      "metadata": {
        "id": "63di7_h6omRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. What is correlation? What does negative correlation mean? ###\n",
        "\n",
        "Ans - In machine learning, correlation is a statistical analysis that measures how strongly two variables are related. It's used to explore data, select features, and build accurate models.\n",
        "\n",
        "In machine learning, negative correlation is when an increase in one variable is associated with a decrease in another variable. It's also known as an inverse correlation."
      ],
      "metadata": {
        "id": "XX6oxGCY06yH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Define Machine Learning. What are the main components in Machine Learning? ###\n",
        "\n",
        "Ans - Machine learning (ML) is a branch of artificial intelligence (AI) that teaches computers to learn from data and perform tasks without explicit programming. The main components of ML are algorithms, data, and computing power.\n",
        "\n",
        "i. **Main components of machine learning**  \n",
        "\n",
        "• **Algorithms**: Provide instructions for processing data.\n",
        "\n",
        "• **Data**: Enables the model to learn patterns and make predictions.\n",
        "\n",
        "• **Computing power**: The power required to run the algorithms and process the data.\n",
        "\n",
        "• **Representation**: How the model looks and how knowledge is represented.\n",
        "\n",
        "• **Evaluation**: How good models are differentiated and programs are evaluated.\n",
        "\n",
        "• **Optimization**: The process for finding good models and generating programs.\n",
        "\n",
        "ii. **Types of machine learning**  \n",
        "\n",
        "• **Supervised learning**: Uses labeled data\n",
        "\n",
        "• **Unsupervised learning**: Finds patterns in unlabeled data\n",
        "\n",
        "• **Semi-supervised learning**: Uses a mix of labeled and unlabeled data\n",
        "\n",
        "• **Reinforcement learning**: Learns from feedback or rewards"
      ],
      "metadata": {
        "id": "ODXUKqVJ1fQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. How does loss value help in determining whether the model is good or not? ###\n",
        "\n",
        "Ans - In statistics and machine learning, loss measures the difference between the predicted and actual values. Loss focuses on the distance between the values, not the direction. For example, if a model predicts 2, but the actual value is 5, we don't care that the loss is negative ( 2 − 5 = − 3 )."
      ],
      "metadata": {
        "id": "pbgtONrA1fNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. What are continuous and categorical variables? ###\n",
        "\n",
        "Ans - In machine learning (ML), continuous variables are numeric values that can take on an infinite number of values within a range. Categorical variables are values that fall into a limited number of categories.\n",
        "\n",
        "i. **Continuous variables**\n",
        "\n",
        "• **Examples**: Height, weight, temperature, length, concentration, age\n",
        "\n",
        "• **Characteristics**: Can take on an infinite number of values within a range\n",
        "\n",
        "• **Type of data**: Numeric\n",
        "\n",
        "ii. **Categorical variables**   \n",
        "\n",
        "• **Examples**: Gender, blood type, political party, type of pet, brand of shoes, agreement rating.  \n",
        "\n",
        "• **Characteristics**: Can take on a limited number of values.\n",
        "\n",
        "• **Type of data**: Can be nominal or ordinal.  \n",
        "\n",
        "iii. **Nominal variables**  \n",
        "\n",
        "• A type of categorical variable that is not ordered.\n",
        "\n",
        "• **Examples**: Hair color, states in the U.S., brands of computers, ethnicities.  \n",
        "\n",
        "iv. **Ordinal variables**   \n",
        "\n",
        "• A type of categorical variable that is ordered.\n",
        "\n",
        "• **Examples**: Education level."
      ],
      "metadata": {
        "id": "xqnBfuHB3EzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. How do we handle categorical variables in Machine Learning? What are the common techniques? ###\n",
        "\n",
        "Ans - Here are some techniques for handling categorical variables in machine learning:\n",
        "\n",
        "• **Ordinal encoding**: Assigns a numerical value to each category based on its rank or position in the order of appearance.\n",
        "\n",
        "• **Label encoding**: Assigns a unique integer to each category in a categorical variable.\n",
        "\n",
        "• **Target encoding**: Converts a categorical value into the mean of the target variable.\n",
        "\n",
        "• **Binary encoding**: Converts categories into binary numbers and splits them into separate columns.\n",
        "\n",
        "• **Frequency encoding**: Converts categorical variables into numerical values by representing each category as the proportion of occurrences of that category in the dataset.\n",
        "\n",
        "• **Dummy encoding**: Also known as one-hot encoding, this technique converts categorical data into a numerical format. It's suitable for nominal categorical features.\n",
        "\n",
        "• **Effect encoding**: Also known as Deviation Encoding or Sum Encoding, this is an advanced categorical data encoding technique.\n",
        "\n",
        "• **Categorical embedding**: A feature engineering method for categorical variables."
      ],
      "metadata": {
        "id": "ImtdR3954ZIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. What do you mean by training and testing a dataset? ###\n",
        "\n",
        "Ans - In machine learning, training and testing datasets are used to teach and evaluate a model's performance.\n",
        "\n",
        "i. **Training data**\n",
        "\n",
        "• Used to teach a model to recognize patterns and perform tasks\n",
        "\n",
        "• The data that the model uses to learn\n",
        "\n",
        "• Typically larger than the testing data\n",
        "\n",
        "• Can be enriched with data labeling or annotation\n",
        "\n",
        "ii. **Testing data**  \n",
        "\n",
        "• Used to evaluate the model's performance and accuracy\n",
        "\n",
        "• Used to see how well the model can predict new answers\n",
        "\n",
        "• Consists of data that the model has never seen before\n",
        "\n",
        "iii. **How to split the data?**  \n",
        "\n",
        "• The data is split into training and testing sets.\n",
        "\n",
        "• The optimal split ratio depends on the complexity of the problem and the learning algorithm.\n",
        "\n",
        "• A common split ratio is 80:20, but other ratios are also used.\n",
        "\n",
        "• The data should be split randomly to avoid biased data.  \n",
        "\n",
        "iv. **Why are training and testing data important?**   \n",
        "\n",
        "• Training and testing data are both important for improving and validating machine learning models.  \n",
        "\n",
        "• Training data influences the model directly, while testing data does not."
      ],
      "metadata": {
        "id": "sf7Bhf-J5NS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. What is sklearn.preprocessing? ###\n",
        "\n",
        "Ans - Sklearn preprocessing in Python is a package that contains tools to transform raw data into a format that machine learning algorithms can use. This process is called data preprocessing.\n"
      ],
      "metadata": {
        "id": "DEG3_i9258Tp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. What is a Test set? ###\n",
        "\n",
        "Ans - A test set in machine learning is a separate set of data that is used to evaluate how well a model performs on new data. It is used to assess the model's accuracy, precision, and recall."
      ],
      "metadata": {
        "id": "krdfm-7m7JDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem? ###\n",
        "\n",
        "Ans - To split data for model fitting in Python, you primarily use the train_test_split function from the scikit-learn library, which divides your dataset into separate training and testing sets; you typically separate your features (X) from target values (y) before using this function, then split both sets into training and testing portions (X_train, X_test, y_train, y_test) to train your model on the training data and evaluate its performance on the unseen testing data.\n",
        "\n",
        "i. **Key steps to split data for model fitting in Python:  \n",
        "\n",
        "• **Import necessary libraries**: Import train_test_split from the sklearn.model_selection module.\n",
        "\n",
        "• **Separate features and target variables**: Identify your independent variables (features) as \"X\" and dependent variable (target) as \"y\" from your dataset.\n",
        "\n",
        "• **Use train_test_split**:\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "• **Explanation**:\n",
        "\n",
        "  • X: Your feature matrix.  \n",
        "\n",
        "  • y: Your target variable.  \n",
        "\n",
        "  • test_size: Proportion of data to allocate to the testing set (e.g., 0.2 for 20%).\n",
        "\n",
        "  • random_state: Sets a seed for random splitting, ensuring reproducibility.  \n",
        "\n",
        "\n",
        "ii. **Approaching a Machine Learning problem**:\n",
        "\n",
        "1. **Define the problem**: Clearly understand the goal of your machine learning task, including the input data, desired output, and evaluation metrics.  \n",
        "\n",
        "2. **Data collection and pre-processing**:\n",
        "\n",
        "  • Gather relevant data.\n",
        "\n",
        "  • Clean and handle missing values.  \n",
        "\n",
        "  • Encode categorical features (if necessary).  \n",
        "\n",
        "  • Feature scaling (if applicable).\n",
        "\n",
        "\n",
        "3. **Data splitting**:\n",
        "\n",
        "  • Use train_test_split to divide your data into training and testing sets.  \n",
        "\n",
        "\n",
        "4. **Choose a model**: Select an appropriate machine learning algorithm based on the problem type (classification, regression, etc.).\n",
        "\n",
        "5. **Model training**:\n",
        "\n",
        "  • Fit the model to the training data (X_train, y_train).  \n",
        "\n",
        "\n",
        "6. **Model evaluation**:\n",
        "\n",
        "  • Make predictions on the test set (X_test).\n",
        "\n",
        "  • Calculate relevant evaluation metrics (accuracy, precision, recall, etc.) to assess model performance.  \n",
        "\n",
        "\n",
        "7. **Hyperparameter tuning**:  \n",
        "\n",
        "  • Adjust model parameters to optimize performance.  \n",
        "\n",
        "\n",
        "8. **Deployment**:\n",
        "\n",
        "  • Once satisfied with the model, use it to make predictions on new data.\n",
        "\n"
      ],
      "metadata": {
        "id": "KSDI6EJe7n-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Why do we have to perform EDA before fitting a model to the data? ###\n",
        "\n",
        "Ans - Before fitting any model, it is often important to conduct an exploratory data analysis (EDA) in order to check assumptions, inspect the data for anomalies (such as missing, duplicated, or mis-coded data), and inform feature selection/transformation."
      ],
      "metadata": {
        "id": "e1OLtP2REdYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. What is correlation? ###\n",
        "\n",
        "Ans - In machine learning, correlation is a statistical analysis that measures how strongly two variables are related. It's used to explore data, select features, and build accurate models.**bold text**"
      ],
      "metadata": {
        "id": "juyJyluhHqfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. What does negative correlation mean? ###\n",
        "\n",
        "Ans - In subject area: Computer Science. 'Negative correlation' in the context of Computer Science refers to a situation where an increase in one variable is associated with a decrease in another variable."
      ],
      "metadata": {
        "id": "m8Yx-9pmH1Ds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. How can you find correlation between variables in Python? ###\n",
        "\n",
        "Ans - To calculate the correlation between two variables in Python, we can use the Numpy corrcoef() function. import numpy as np np. random. seed(100) #create array of 50 random integers between 0 and 10 var1 = np."
      ],
      "metadata": {
        "id": "H0MnDYvhJ5qj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. What is causation? Explain difference between correlation and causation with an example. ###\n",
        "\n",
        "Ans - Causation is when one event directly causes another, while correlation is when two things happen together but one doesn't cause the other.\n",
        "\n",
        "**Explanation**\n",
        "\n",
        "i. **Causation**\n",
        "\n",
        "A stronger statement than correlation, causation means that one event is the result of another. For example, hitting a billiard ball with a cue stick causes the ball to move.  \n",
        "\n",
        "ii. **Correlation**\n",
        "\n",
        "A relationship between two variables, but one event doesn't necessarily cause the other. For example, ice cream sales and pool drownings are correlated because both increase in the summer, but ice cream doesn't cause people to drown.\n",
        "\n",
        "**Examples**  \n",
        "\n",
        "• **Smoking and alcoholism**: Smoking is correlated with alcoholism, but it doesn't cause alcoholism.\n",
        "\n",
        "• **Exercise and skin cancer**: There may be a correlation between exercise and skin cancer, but it's not clear if exercise causes skin cancer.\n",
        "\n",
        "• **Mango sales and air conditioner sales**: Mango and air conditioner sales are correlated because both increase in the summer, but warmer weather is the cause."
      ],
      "metadata": {
        "id": "JuDBWJyjKNbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16. What is an Optimizer? What are different types of optimizers? Explain each with an example. ###\n",
        "\n",
        "Ans - **In Machine Learning, an optimizer is an algorithm that adjusts the parameters (weights and biases) of a model during the training process.**\n",
        "\n",
        "The goal of an optimizer is to minimize the model's error or loss function. It does this by iteratively updating the parameters based on the gradient of the loss function.\n",
        "\n",
        "**Here are some of the most common types of optimizers:**\n",
        "\n",
        "**1. Gradient Descent:**\n",
        "\n",
        "    * **Concept:** The most basic optimizer. It updates the parameters in the direction of the steepest descent of the loss function.\n",
        "\n",
        "    * **Example:** Imagine a hiker trying to find the lowest point in a valley. Gradient descent would be like the hiker always taking the steepest downhill step.\n",
        "\n",
        "**2. Stochastic Gradient Descent (SGD):**\n",
        "\n",
        "    * **Concept:** Instead of calculating the gradient of the entire dataset, SGD calculates the gradient on a small batch of data (a subset of the training data).\n",
        "\n",
        "    * **Example:** Instead of looking at the entire landscape of the valley, the hiker only looks at a small portion around their current location to decide which direction to take.\n",
        "\n",
        "    * **Advantages:** Faster training, can escape local minima more easily.\n",
        "\n",
        "**3. Adam:**\n",
        "\n",
        "    * **Concept:** Combines the advantages of AdaGrad and RMSprop. It computes adaptive learning rates for each parameter, making it efficient and well-suited for a wide range of problems.\n",
        "\n",
        "    * **Example:** A more sophisticated hiker who not only considers the current slope but also remembers past steps and adjusts their pace accordingly.\n",
        "\n",
        "**4. RMSprop:**\n",
        "\n",
        "    * **Concept:** Adapts the learning rate for each parameter based on the historical average of the squared gradients.\n",
        "\n",
        "    * **Example:** The hiker pays more attention to directions where they have previously encountered steep slopes.\n",
        "\n",
        "**5. AdaGrad:**\n",
        "\n",
        "    * **Concept:** Adaptively adjusts the learning rate for each parameter based on the sum of the squares of the gradients.\n",
        "\n",
        "    * **Example:** The hiker slows down significantly in areas where the terrain is very steep.\n",
        "\n",
        "**Choosing the right optimizer:**\n",
        "\n",
        "The choice of optimizer depends on various factors, including:\n",
        "\n",
        "* **The complexity of the model:**\n",
        "\n",
        "* **The size of the dataset:**\n",
        "\n",
        "* **The specific characteristics of the problem.**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YDEE4J-HRxXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 17. What is sklearn.linear_model ? ###\n",
        "\n",
        "Ans - sklearn.linear_model provides a valuable set of tools for implementing various linear regression models in Python. These models are widely used for predictive modeling, and the choice of the specific model depends on the characteristics of the data and the specific requirements of the problem."
      ],
      "metadata": {
        "id": "-gSl_yr5Tugx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 18. What does model.fit() do? What arguments must be given? ###\n",
        "\n",
        "Ans - Certainly! In scikit-learn, the `model.fit()` method is the core function used for training a machine learning model. It takes the training data as input and uses it to learn the patterns and relationships within the data. These learned patterns are then used to make predictions on new, unseen data.\n",
        "\n",
        "**Arguments for `model.fit()`:**\n",
        "\n",
        "* **X (mandatory):** This argument represents the features or independent variables of your training data. It should be a 2D array where each row represents a data sample and each column represents a feature.\n",
        "\n",
        "* **y (mandatory for supervised learning):** This argument represents the target variable or labels of your training data. It can be a 1D array (for regression problems) or a 2D array (for multi-class classification problems).\n",
        "\n",
        "* **sample_weight (optional):** This argument allows you to assign weights to individual samples in the training data. This can be useful if you want to emphasize certain samples during training.\n",
        "\n",
        "* **Other parameters (optional):** Depending on the specific machine learning model you're using, there might be other optional parameters you can provide to `model.fit()`. These parameters can control aspects of the training process, such as the learning rate, the number of training epochs, or the regularization technique used.\n",
        "\n",
        "Here's a breakdown of what happens during `model.fit()`:\n",
        "\n",
        "1. **Data Preprocessing (internal):** The model might perform some internal preprocessing steps on the training data (X and y) you provide. This could involve scaling or normalizing the features, handling missing values, or encoding categorical variables.\n",
        "\n",
        "2. **Learning Algorithm:** The core training process happens here. The specific algorithm used depends on the type of model you're using (e.g., linear regression, decision tree, support vector machine). The algorithm iteratively updates the model's internal parameters (weights and biases) based on the training data.\n",
        "\n",
        "3. **Loss Function Optimization:** During training, the model tries to minimize a loss function that measures the difference between the model's predictions and the actual target values (y)."
      ],
      "metadata": {
        "id": "CPHFBdfYUfs-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 19. What does model.predict() do? What arguments must be given? ###\n",
        "\n",
        "Ans - In scikit-learn, `model.predict()` is the method used to make predictions on new, unseen data using a trained machine learning model.\n",
        "\n",
        "**What it does:**\n",
        "\n",
        "* **Uses Learned Patterns:** `model.predict()` leverages the patterns and relationships learned by the model during the training phase (using `model.fit()`) to generate predictions for new input data.\n",
        "\n",
        "* **Input:** It takes as input the features of the new data points you want to make predictions for. This input should have the same format (number of features) as the training data used to fit the model.\n",
        "\n",
        "* **Output:** It returns the predicted output values for each of the input data points. The format of the output depends on the type of problem (regression, classification, etc.).\n",
        "\n",
        "**Arguments:**\n",
        "\n",
        "* **X (mandatory):** This argument represents the features of the new data points for which you want to make predictions. It must have the same number of features as the training data used to fit the model.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "`model.predict()` is a crucial step in the machine learning workflow. It allows you to use your trained model to make predictions on new, unseen data, which is the ultimate goal of most machine learning projects.\n"
      ],
      "metadata": {
        "id": "hzkMZYeQWLCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 20. What are continuous and categorical variables? ###\n",
        "\n",
        "Ans - **Continuous Variables**\n",
        "\n",
        "* **Definition:** These variables can take on any value within a given range.\n",
        "\n",
        "* **Characteristics:**\n",
        "\n",
        "    * Often measured on a scale that has an infinite number of possible values between any two points.\n",
        "\n",
        "    * Usually represented by numbers.\n",
        "\n",
        "**Categorical Variables**\n",
        "\n",
        "* **Definition:** These variables represent categories or groups.\n",
        "\n",
        "* **Characteristics:**\n",
        "\n",
        "    * Have a finite number of distinct values.\n",
        "\n",
        "    * Often represented by labels or names.\n"
      ],
      "metadata": {
        "id": "8tTPFInfYIHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 21. What is feature scaling? How does it help in Machine Learning? ###\n",
        "\n",
        "Ans - **Feature Scaling**\n",
        "\n",
        "* **Definition:** Feature scaling is a crucial data preprocessing technique in machine learning that involves transforming the numerical features of a dataset to a common scale or range.\n",
        "\n",
        "* **Why is it important?**\n",
        "\n",
        "    * **Improves model performance:**\n",
        "    \n",
        "        * **Convergence:** Many machine learning algorithms, especially gradient descent-based algorithms, converge faster and more reliably when features are on a similar scale. Features with larger magnitudes can dominate the learning process, slowing down convergence and potentially leading to suboptimal solutions.\n",
        "\n",
        "        * **Stability:** Scaling can make the model more robust to changes in the input data distribution.\n",
        "\n",
        "        * **Improved accuracy:** By ensuring that all features contribute equally to the model's predictions, scaling can significantly improve the model's accuracy and generalization performance.\n",
        "\n",
        "    * **Prevents bias:** Features with larger values can have a disproportionate influence on the model, leading to biased predictions. Scaling helps to prevent this bias by ensuring that all features are treated equally.\n",
        "\n",
        "* **Common Scaling Techniques:**\n",
        "\n",
        "    * **Standardization (Z-score normalization):**\n",
        "\n",
        "        * Transforms features to have zero mean and unit variance.\n",
        "\n",
        "        * Formula: `(x - mean) / standard deviation`\n",
        "\n",
        "\n",
        "    * **Min-Max Scaling:**\n",
        "\n",
        "        * Transforms features to a specific range, typically between 0 and 1.\n",
        "\n",
        "        * Formula: `(x - min(x)) / (max(x) - min(x))`\n",
        "\n",
        "    * **Robust Scaling:**\n",
        "\n",
        "        * Less sensitive to outliers than other methods.\n",
        "        \n",
        "        * Uses the interquartile range (IQR) to scale the data."
      ],
      "metadata": {
        "id": "vnDsBxFlY6O0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 22. How do we perform scaling in Python? ###\n",
        "\n",
        "Ans - **Explanation:**\n",
        "\n",
        "1. **Import necessary libraries:**\n",
        "\n",
        "   - `StandardScaler`: For standardizing the data.\n",
        "\n",
        "   - `MinMaxScaler`: For performing min-max scaling.\n",
        "\n",
        "2. **Create sample data:**\n",
        "\n",
        "   - Replace this with your actual dataset.\n",
        "\n",
        "3. **Standardization:**\n",
        "\n",
        "   - Create an instance of `StandardScaler()`.\n",
        "\n",
        "   - Use `fit_transform()` to standardize the data. This method first calculates the mean and standard deviation of each feature and then transforms the data accordingly.\n",
        "\n",
        "4. **Min-Max Scaling:**\n",
        "\n",
        "   - Create an instance of `MinMaxScaler()`.\n",
        "\n",
        "   - Use `fit_transform()` to normalize the data. This method scales the data to a range between 0 and 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "_tnSzPXbaY23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "data = [[1, 2, 3],\n",
        "        [4, 5, 6],\n",
        "        [7, 8, 9]]\n",
        "\n",
        "# 1. Standardization (Z-score normalization)\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(\"Standardized Data:\\n\", scaled_data)\n",
        "\n",
        "# 2. Min-Max Scaling (Normalization)\n",
        "scaler = MinMaxScaler()\n",
        "normalized_data = scaler.fit_transform(data)\n",
        "print(\"\\nNormalized Data:\\n\", normalized_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Imx0lXFqa3Gp",
        "outputId": "1d56e4e6-2433-4183-d734-3bfef898e81b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized Data:\n",
            " [[-1.22474487 -1.22474487 -1.22474487]\n",
            " [ 0.          0.          0.        ]\n",
            " [ 1.22474487  1.22474487  1.22474487]]\n",
            "\n",
            "Normalized Data:\n",
            " [[0.  0.  0. ]\n",
            " [0.5 0.5 0.5]\n",
            " [1.  1.  1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 23. What is sklearn.preprocessing? ###\n",
        "\n",
        "Ans - In scikit-learn, `sklearn.preprocessing` is a crucial submodule that provides a wide range of techniques for transforming raw data into a format suitable for machine learning algorithms.\n"
      ],
      "metadata": {
        "id": "I85tSHeUa52V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 24. How do we split data for model fitting (training and testing) in Python? ###\n",
        "\n",
        "Ans - **Explanation:**\n",
        "\n",
        "* **`train_test_split(X, y, test_size=0.2, random_state=42)`**\n",
        "\n",
        "    * `X`: Features of your data.\n",
        "\n",
        "    * `y`: Target variable.\n",
        "\n",
        "    * `test_size=0.2`: Specifies that 20% of the data will be used for the test set, and 80% for the training set. You can adjust this value as needed.\n",
        "    \n",
        "    * `random_state=42`: This ensures that the data is split in the same way every time you run the code with the same `random_state` value. This is important for reproducibility of your results.\n",
        "\n",
        "**Key Considerations:**\n",
        "\n",
        "* **Data Size:** A common split is 80% for training and 20% for testing. However, this can vary depending on the size of your dataset.\n",
        "\n",
        "* **Stratified Sampling:** For imbalanced datasets (where the classes are not equally represented), consider using `stratify=y` within `train_test_split()`.\n",
        "\n",
        "This ensures that the class proportions in the training and testing sets are maintained.\n",
        "\n",
        "* **Multiple Splits:** For more robust evaluation, you can perform multiple train-test splits (e.g., using k-fold cross-validation) and average the results to get a more reliable estimate of your model's performance.\n",
        "\n",
        "By using `train_test_split()` effectively, you can ensure a fair and unbiased evaluation of your machine learning models and gain confidence in their ability to generalize to new, unseen data.\n"
      ],
      "metadata": {
        "id": "Z0tpFDfabswQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming you have your data as:\n",
        "X =  # Your features\n",
        "y =  # Your target variable\n",
        "\n",
        "# Split the data (example: 80% for training, 20% for testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "j-iNgvN0caJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 25. Explain data encoding? ###\n",
        "\n",
        "Ans - In machine learning, data encoding is a crucial preprocessing step that involves converting categorical data into a numerical format. This is necessary because most machine learning algorithms require numerical input."
      ],
      "metadata": {
        "id": "KuqSR-PXcis_"
      }
    }
  ]
}